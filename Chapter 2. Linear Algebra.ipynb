{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "p1 (**page31**):\n",
    "\n",
    "* 선형대수학은 수학의 한부분으로서 과학과 엔지니어링에 많이 쓰임\n",
    "* 선형대수학은 연속적인 문제를 다루고 있음\n",
    "* 머신러닝 알고리즘을 이해하기 위해서는 선형대수학을 이해하는것이 필수적임\n",
    "\n",
    "p2:\n",
    "\n",
    "* 선형대수학에 이미 많이 알고있다면 이챕터를 뛰어넘어도 됨\n",
    "* 그 외 참고자료 이야기...\n",
    "\n",
    "## 2.1 Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "#### 스칼라(Scalars): 단순하게 하나의 숫자를 말하며 보통 변수를 표현할떄 소문자로 나타냄\n",
    "#### 벡터(Vectors)\n",
    "\n",
    "   * 벡터는 숫자들의 배열로써 배열에 있는 숫자들은 각각 인덱스넘버를 가지고 이를 이용해서 정렬됨 \n",
    "   * 벡터는 소문자영어에 볼드체로 표현함\n",
    "   * 만약 벡터의 수가 \\\\(\\mathbb{R} \\\\) 안에 있고 그 벡터가 n개 가지고 있다면, \\\\(\\mathbb{R} \\\\)을 n번 곱한 \\\\({\\mathbb{R}}^{n} \\\\)로 표      현됨\n",
    "    \n",
    "\n",
    "\\\\(\\mathbb{R} \\\\)\n",
    "\\\\(exp_a b \\\\) \n",
    "\n",
    "\\\\( x(t)=\\frac{-b\\pm \\sqrt{{b}^{2}-4ac}}{2a} \\\\)\n",
    "\n",
    "\\\\( \\sqrt{{b}^{2}-4ac} \\\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![add_2_1](https://user-images.githubusercontent.com/52661707/75309753-53f29680-5895-11ea-9550-72cfdde6692b.PNG)\n",
    "\n",
    "※ https://ko.wikipedia.org/wiki/%EA%B3%B1%EC%A7%91%ED%95%A9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 벡터의 일부 숫자들을 다시 하나의 그룹으로 만들 수 있음\n",
    "* 예를들어 \\\\(x_1 \\\\), \\\\(x_3 \\\\), \\\\(x_6 \\\\)을 하나의 그룹으로 만들고싶으면 S = {1, 3, 6}으로 표현하고 \\\\(x_S \\\\)라고 표현할 수도 있음\n",
    "* 만약 일부분을 제외한 모든 요소를 표현하고 싶으면 - 을 쓰면 됨\n",
    "* 예를들어 S를 벡터 x에서 제외하고 싶으면 \\\\(x_-s \\\\)로 표현할 수 있음\n",
    "\n",
    "#### 배열(Matrices)\n",
    "   * 숫자들의 2차원 배열을 의미하며 따라서 벡터와 다르게 두개의 인덱스를 가지고 있음\n",
    "   * 스칼라는 벡터와 다르게 대문자영어라 볼드체로 표현함\n",
    "   * 배열 A가 m개의열과 n개의 행을 가지고 있으면  A\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{nxm} \\\\)으로 표현\n",
    "   * 배열의 숫자를 나타낼때 \\\\(A_{m,n} \\\\)으로 나타냄\n",
    "   * 파이썬처럼 콜른(:)을 이용하여 행 또는 열의 모든 숫자라는 의미를 나타낼 수 있음\n",
    "   * 예를 들어, \\\\(A_{i,:} \\\\)는 i번째 행의 모든 숫자들을 나타냄\n",
    "   * \\\\(f(A)_{m,n} \\\\)는 (m,n)에 해당하는 숫자를 함수f를 통해 나온 결과값을 나타냄\n",
    "   \n",
    "#### 텐서(Tensors)\n",
    "   * 2차원을 넘어 3차원의 배열을 텐서하고 함\n",
    "\n",
    "p1 (**page33**):\n",
    "   * 배열이용하여 할 수 있는 것 중 중요한 기능은 전치(transpose)임\n",
    "   * 배열의 x,y순서를 y,x순서로 바꾸는 행위를 말함\n",
    "   * \\\\(({A}^{T})_{i,j} \\\\) = \\\\({A}_{j,i} \\\\)로 표현됨\n",
    "   \n",
    "p2: \n",
    "   * 하나의 열을 전치시키면 하나의 행이 됨\n",
    "   * 예를 들면 x = \\\\({[x_1, x_2]}^{T} \\\\) 는 \\\\({x_1 \\choose x_2} \\\\)\n",
    "   \n",
    "p1 (**page34**):\n",
    "   * 스칼라를 전치시키면 스칼라다\n",
    "   \n",
    "p2:\n",
    "   * 배열의 크기가 같다면, \\\\({C}_{j,i} = \\\\) + \\\\({A}_{j,i} \\\\) + \\\\({B}_{j,i} \\\\) 대신에 C = A + B라고 사용할 수도 있음\n",
    "   \n",
    "p3:\n",
    "   * 스칼라와 배열은 서로 곱할수도 있고 더할수도 있음\n",
    "\n",
    "p4:\n",
    "   * 배열과 벡터의 합도 가능하다\n",
    "\n",
    "p5:\n",
    "   * \\\\({C}_{j,i} = \\\\) + \\\\({A}_{j,i} \\\\) + \\\\({b}_{j} \\\\)는 벡터 b가 모든 행에 더해지는 것을 의미하고 이때 벡터 b의 값이 행의 모든 값에      각각 더해지기 때문에 broadcast라 하기도 함\n",
    "   \n",
    "\n",
    "## 2.2 Multiplying Matrics and Vectors\n",
    "\n",
    "p5:\n",
    "   * 배열의 가장 중요한 기능중 하나는 두 배열간의 곱셈임\n",
    "   * 두 배열을 곱할때는 열과 행의 수가 같아야 함\n",
    "   * 예를 들면 \\\\({C}_{j,i} = \\\\) + \\\\({A}_{i,k} \\\\) = \\\\({B}_{j,k} \\\\)\n",
    "   \n",
    "p6:\n",
    "   * 보통 하는 행렬의 곱 말고 아다마르곱(Hadamard product)이라는 방법이 존재함\n",
    "   * 두 행렬 A와 B의 아다마르곱의 표현읜 A⊙B로 나타내며 두 행렬의 같의 위치의 값을 서로 곱함\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![add_2_2](https://user-images.githubusercontent.com/52661707/75309829-7e445400-5895-11ea-96a3-655bacc881ef.PNG)\n",
    "\n",
    "※ https://ko.wikipedia.org/wiki/%EC%95%84%EB%8B%A4%EB%A7%88%EB%A5%B4_%EA%B3%B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "   * 스칼라곱(dot product)는 두개의 같은 차원을 가진 두개의 백터곱으로 다음과 같이 표현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![add_2_3](https://user-images.githubusercontent.com/52661707/75309830-7edcea80-5895-11ea-9c4d-b15b3f10110f.PNG)\n",
    "\n",
    "※ https://ko.wikipedia.org/wiki/%EC%95%84%EB%8B%A4%EB%A7%88%EB%A5%B4_%EA%B3%B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page35**):\n",
    "   * 배열의 곱은 분배법칙과 결합법칙이 가능함\n",
    "   * 하지만 교환법칙은 성립하지 않음 (AB != BA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_1](https://user-images.githubusercontent.com/52661707/75309835-80a6ae00-5895-11ea-9fad-b29e687107ef.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "   * 배열의 전치에 있어서 분배법칙이 일어날때는 \\\\({(AB)}^{T} = {B}^{T}{A}^{T}\\\\)\n",
    "   * 이 법칙에 따라 \\\\({A}^{T}{B} = {({A}^{T}B)}^{T} = {B}^{T}{A}\\\\) 가 성립함\n",
    "   \n",
    "p3:\n",
    "   * 이 책이 선형대수학책이 아니기 때문에 더이상 깊게 가지는 않겠지만, 독자들은 더 많은 것들을 알고 있어야함\n",
    "\n",
    "p4:\n",
    "   * 이쯤되면 선형대수학에 대해 어느정도 안다고 할 수 있음\n",
    "   * Ax = b라는 식에서 A는 알고있는 행렬 A\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{mxn} \\\\)이고 b는 알고있는 벡터 b\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{m} \\\\)이고 x는 b\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{n} \\\\)으로 알 수 없는 벡터일 때, 알수 없는 벡터 \\\\(x_i \\\\)에 대해 알고자 할 때 다음과 같이 나타낼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_2](https://user-images.githubusercontent.com/52661707/75309837-80a6ae00-5895-11ea-97e2-5f05c4414c38.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2 (**page36**):\n",
    "   * 행렬 벡터 곱을 이용하여 좀 더 간결하게 나타낼 수 있음\n",
    "\n",
    "\n",
    "## 2.3 Identity and Inverse Matrices\n",
    "선형대수학은 역행렬을 이용해서 A의 수많은 값들에 대한 식 2.11을 해결할 수 있게 함\n",
    "\n",
    "p2:\n",
    "   * 역행렬을 설명하기 위해 먼저 단위행렬(Identiry matrix)에 대해 알아야 함\n",
    "   * 단위행렬은 대각선의 값이 1인 행렬을 말하며 벡터가 행렬과 곱했는데 벡터의 변화가 생기지 않게하는 행렬을 말함\n",
    "   * n차원의 단위행렬은 \\\\({I}_n \\\\)으로 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_3](https://user-images.githubusercontent.com/52661707/75309838-813f4480-5895-11ea-97ac-1d717f70df3f.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "   * 역행렬 A는 \\\\({A}^{-1} \\\\)로 표현하고 \\\\({A}^{-1} {A}\\\\) = \\\\(I_n \\\\)으로 정의됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "   * 식 2.11은 역행렬을 이용하여 다음과 같이 나열할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_4](https://user-images.githubusercontent.com/52661707/75309839-813f4480-5895-11ea-9d22-ab55a4ea4318.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_5](https://user-images.githubusercontent.com/52661707/75309840-81d7db00-5895-11ea-8d7d-0902bca077c9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page37**):\n",
    "   * 이 때, 역행렬이 존재하는 A에 한해서면 가능함\n",
    "   \n",
    "p2:\n",
    "   * 역행렬 A에 대한 제한점에 대해 설명\n",
    "   * 역행렬은 항상 존재하는 것이 아니라 상황에 따라 있을 수도 없을 수도 있기 때문에 디지털 컴퓨터에서 제한된 상황을 나타낼 수 있어서 \n",
    "   * A의 역행렬이 존재할 때, A의 역행렬은 다른 b값들과 많이 계산되는데, 이 때, A의 역행렬은 이론상의 값이고, S/W 어플리케이션에서 실제값으로 사용될 수는 없음\n",
    "   * 왜냐하면 A의 역행렬은 한정된 환경에서만 존재하기 때문에 b를 사용하는 알고리즘이 보통 더 정확한 x 예측값을 얻을 수 있음\n",
    "   \n",
    "## 2.4 Linear Dependence and Span\n",
    "역행렬 A가 존재하기 위해서는 식 2.11에서 모든 b에 대한 해는 항상 같은 값이 나와야하지만, 일부 해가 없거나 무한히 많은 해를 가지는 식의 시스템에서도 가능함\n",
    "특정한 변수 b에 대해 한개 이상의 값의 값을 가질수는 없지만 무한히 많은 값보다는 적은 수의 값을 가질 수 있음\n",
    "만약 x, y가 불특정한 해라고 할 때,\n",
    "                                        z = \\\\({a} \\\\)x + (1-\\\\({a} \\\\))y\n",
    "는 어떠한 \\\\({a} \\\\)를 이용하여 해결할 수 있음\n",
    "\n",
    "p3:\n",
    "   * 얼마나 많은 해가 있는지 알아보기 위해, A의 열을 원점에서 출발하는 서로 다른 방향이라고 생각할 떄, b로 도달 할 수 있는 길의 수에 대해 결정함\n",
    "   * 이 때, x를 특정방향에 대해 얼마나 멀리 이동했는지에 대한 벡터로 생각할 수 있음\n",
    "   * 그럼 다음과 같이 수식으로 만들 수 있음\n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_6](https://user-images.githubusercontent.com/52661707/75309841-81d7db00-5895-11ea-9c3c-988aa7858136.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 이 떄 이것을 선형 결합(linear combination)이라 함\n",
    "   * 선형 결함은 스칼라배와 벡터 덧셈을 통해 새로운 벡터를 얻는 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![add_2_4](https://user-images.githubusercontent.com/52661707/75309831-7edcea80-5895-11ea-815a-513ab09c48ca.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page38**):\n",
    "   * ax = b 가 해를 가지는지 판단하는 것은 b가 A의 열 범위에 있는지 여부를 확인하는 것과 같음\n",
    "   \n",
    "p2:\n",
    "   * ax = b가 모든 b( b\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{m} \\\\))의 값에서 해를 가지기 위해서는 A열 또한  A\\\\(\\in \\\\)\\\\({\\mathbb{R}}^{m} \\\\)여야함\n",
    "   * 만약 \\\\({\\mathbb{R}}^{m} \\\\)의 크기를 초과하게 된다면, 그 초과된 벡터 위치의 값은 아무런 값을 가지지 않음\n",
    "   * 즉, 행렬을 열의 갯수(n)이 벡터의 갯수(m)보다 클 떄\n",
    "   * 따라서 행렬 A는 최소한 m개의 열(n)을 가지고 있어야함 --> n >= m\n",
    "\n",
    "p3:\n",
    "   * n >= m이라도 n의 일부분이 중복될 수 있어서 조건이 충분하지 않을 수 있음\n",
    "   * 즉, n의 갯수는 독립적인 관계를 가진 열벡터의 수이며 다른 말로 기저벡터의 수를 말함\n",
    "   \n",
    "p4:\n",
    "   * 이런 벡터들의 중복성은 선형 종속성으로 말할 수 있음\n",
    "   * 벡터가 다른 벡터의 선형조합이 아니라면 이 경우 벡터는 선형 독립이라 함\n",
    "   * 3번쨰 단락과 같이 벡터들이 서로 선형조합이면, 열공간에 포함시키지 않음\n",
    "   \n",
    "\n",
    "p5:\n",
    "   * 행렬이 역행렬을 가지려면 2.11식에서 b의 값에 대해 하나의 해가 있는지 확인해야함\n",
    "   * 그러기 위해선 행렬에 최대 m개의 열이 있어야함\n",
    "   \n",
    "p6:\n",
    "   * 행과 열이 같은 정사각형 행렬이 선형독립성을 가지고 있다면, 이것을 Singular라 부흠\n",
    "   * 이런 상황에서 역행렬을 구할 수 있고 그에 따른 해를 구할 수 있음\n",
    "   \n",
    "p7:\n",
    "   * 만약 A가 정사각행렬이 아니거나 정사각행렬인데 Singular가 아니라도, 해를 구할 수 있는 방법은 있지만, 역행렬을 이용해서는 구할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_11](https://user-images.githubusercontent.com/52661707/75309847-83a19e80-5895-11ea-9bcb-c3c2b6abdfdd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_10](https://user-images.githubusercontent.com/52661707/75309845-83090800-5895-11ea-8d04-513437d2d0a8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ https://m.blog.naver.com/PostView.nhn?blogId=jerrypoiu&logNo=221506739101&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_7](https://user-images.githubusercontent.com/52661707/75309842-82707180-5895-11ea-9908-aa6b0795340a.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 위의 경우 두 벡터의 관계는  종속일까 독립일까?\n",
    "   * 두 벡터의 관계는 종속\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eq_2_9](https://user-images.githubusercontent.com/52661707/75309844-83090800-5895-11ea-8115-de97d02dcc9d.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 두 벡터의 관계는 종속일까 독립일까?\n",
    "   두 벡터의 관계는 독립\n",
    "   \n",
    "   * 벡터들 간의 종속 독립 관계는 추후 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 39**):\n",
    "+ 벡터는 수의 배열로 표현 되며 크기를 구할 수 있는데, 이 값을 Norm(놈)이라 부름\n",
    "+ 수식적으로 $L^{p}$ Norm은 다음과 같이 정의된다. $$||x||_p = \\left(\\sum_i { |x_i|^{p} } \\right)^{\\frac{1}{p}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ Norm은 원점으로부터의 거리로 해석할 수 있으며, 다음 3가지 공식을 만족 해야 한다.\n",
    "\n",
    "$$f(x) = 0 \\implies \\textbf{x} = \\textbf{0} $$\n",
    "$$f(x + y) \\leq f(x) + f(y) \\;\\;(삼각 부등식)$$\n",
    "$$\\forall\\alpha, f(\\alpha x) = |\\alpha|f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 여기서 p=1 일 때의 Norm을 \"맨해탄 Norm\"이라 부르며, $L^{1}$ 으로 나타낼 수 있다.\n",
    "+ 여기서 p=2 일 때의 Norm을 \"유클리디언 Norm\"이라 부르며, $L^{2}$ 으로 나타낼 수 있다.\n",
    "+ 이 $L^{2}$기계학습에서 매우 자주 사용되며, 수식으로 나타낼 때 에도 $||x||_2$ 가 아니라 2를 생략하고 $||x||$ 로 표현한다.\n",
    "+ 수식적으로 간단히 $x^{\\top} x$로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 실제 기계학습에서 $L^{2}$를 사용 할 때는 결과에 제곱을 해서 사용한다.\n",
    "+ $L^{2}$는 직관적으로 제곱을 하여 사용하기 때문에 outlier(이상점)의 정도가 민감하게 바뀐다. 1을 기준으로 적다면 적게 바뀌고 많다면 많이 바뀐다.\n",
    "+ $L^{1}$은 머신러닝에서 에러를 구하는 방법 중 하나인 최소절대편차(Least Absolute Deviations, LAD)에 사용된다.\n",
    "+ $L^{2}$는 머신러닝에서 에러를 구하는 방법 중 하나인 최소제곱오차(Least Squares Error)에 사용된다.\n",
    "\n",
    "$$||x||_1 = \\sum_i { |x_i| }  $$\n",
    "$$||x||_1 = |x_1| + |x_2| + |x_3| + ... + |x_i|$$\n",
    "$$S = \\sum_{i=1}^n |{y_i - f(x_i) }|$$\n",
    "\n",
    "\n",
    "$$||x||_2 = \\left(\\sum_i { |x_i|^{2} } \\right)^{\\frac{1}{2}}  $$\n",
    "$$||x||_2 = \\sqrt{|x_1|^{2} + |x_2|^{2} + |x_3|^{2} + ... + |x_i|^{2}}$$\n",
    "$$S = \\sum_{i=1}^n \\left({y_i - f(x_i) }\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2 (**page 40**):\n",
    "+ $L^{1}$은 outlier(이상치)의 변화에 '적당히' 영향을 미친다.\n",
    "+ 그렇기 때문에 0과 0이 아닌 요소의 차이가 매우 중요한 경우에 사용할 수 있다.\n",
    "+ 항상 0점으로 부터 $\\epsilon$ 만큼 증가 할 때 똑같이 $\\epsilon$ 만큼 증가한다. (선형성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ $L^{0}$은 열벡터에서 0이 아닌 요소의 수를 세는데 사용된다.\n",
    "+ 하지만 $L^{0}$은 올바른 표현이 아니다. 3번째 공식을 만족하지 않으므로 이 기능이 Norm이라 표현하는 것은 오류다.\n",
    "+ $L^{1}$는 0이 아닌 요소의 수를 대체하는데 사용된다. (이해 안됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 열벡터 내부에 가장 큰 요소를 뽑아 내는 것은 $L^{\\infty}$(max norm)이라 한다. \n",
    "\n",
    "$$||x||_\\infty = \\underset{i}{max} { |x_i| }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬의 크기를 구하는 Norm을 Frobenius norm이라 한다.\n",
    "\n",
    "$$||A||_F = \\left(\\sum_{i,j} { A^{2}_{i,j} } \\right)^{\\frac{1}{2}}  $$\n",
    "\n",
    "+ $L^{2}$와 비슷함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 두 벡터의 내적의 계산은 Norm으로 표현 가능하며, x 벡터와 y 벡터가 $\\theta$ 각도를 이룰 때 다음과 같다.\n",
    "\n",
    "$$x^{\\top}y = ||x||_2||y||_2\\cos{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Special Kinds of Matrices and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ 일부 유용한 특수 행렬과 특수 벡터가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 대각행렬 $D$\n",
    "+ $i \\neq j$인 경우 $D_{i,j} = 0$ 이다.\n",
    "+ 대표적으로 항등행렬 $I$(또는 $E$)가 있음\n",
    "+ 대각 행렬만 뽑아내 벡터로 만들었을 때 그 벡터를 $v$라고 한다면, 대각행렬의 순서가 $v$와 같은 배열을 $diag(v)$라고 한다.\n",
    "+ 이는 한 벡터에 각각의 다른 고유한 숫자를 곱할 때 유용하다.\n",
    "\n",
    "+ 행렬 $A$이 대각 행렬이고, 대각선 성분이 $v=[1,2,3]^{\\top}$일 경우, $A$는 $diag(v)$로 표현될 수 있다.\n",
    "+ 벡터 $x=[2,4,6]^{top}$과 $diag(v)$를 곱하면, 벡터 $v$와 벡터 $x$의 각 요소를 곱한 것과 같다.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "diag(v)x & = & [v_1x_1, v_2x_2, v_3x_3] \\\\\n",
    "& = & [1 \\times 2, 2 \\times 4, 3 \\times 6]\n",
    "\\end{eqnarray}\n",
    "\n",
    "+ 더욱 간단히 $v \\odot x$으로 표현할 수 있다.\n",
    "+ $diag(v)$의 역함수는 $diag(v)^{-1}$로 표현하며, $diag(v)^{-1}= diag([1/v_1, 1/v_2, ... , 1/v_n])$으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 41**):\n",
    "+ 대각행렬 $D$가 꼭 정사각행렬일 필요는 없다.\n",
    "+ 정사각행렬이 아닌 $D$는 역행렬이 존재하지 않는다.\n",
    "+ 곱해지는 벡터 $x$의 일정 수치의 고차원 요소를 0으로 만들고자 할 때(스케일링) 사용한다. ($D$가 $i \\times j$ 행렬 일 때, $i$개 이상의 요소는 전부 0이 된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 대칭행렬(Symmetric matrix)는 행렬에 트랜스포즈(Transpose : $A^{\\top}$) 연산을 수행 해도 원래의 $A$와 같은 행렬을 말한다.\n",
    "$$A = A^{\\top}$$\n",
    "+ 무조건 정사각행렬 이어야 하며, $A$의 모든 원소에 대해 $A_{i,j} = A_{j,i}$를 만족하는 행렬을 말한다.\n",
    "+ 일반적으로 어떠한 규칙에 따라 행렬을 만들 때 순서에 상관 없는 경우 대칭행렬이 만들어진다.\n",
    "+ 예를 들어 단순이 좌표평면 $(i,j)$의 거리를 나타내는 행렬을 만들 경우 $i$에서 $j$의 거리나, $j$에서 $i$의 거리는 같기 때문에 대칭행렬로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 단위 벡터(unit vector)는 단위 노름(unit norm)으로 정의된다.\n",
    "$$||x||_2 = 1  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ $x^{\\top}y = 0$인 경우, 두 벡터 $x$와 $y$는 서로 **직교** 한다고 말한다. \n",
    "+ 두 벡터가 직교 하면서 둘 다 단위 벡터라면, 이를 정규직교라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 직교 행렬은 열벡터들과 행벡터들이 서로 직교하는 행렬을 말한다.\n",
    "+ 따라서 다음의 식을 만족한다.\n",
    "\n",
    "$$A^{\\top}A = AA^{\\top} = I$$\n",
    "+ 즉,\n",
    "$$A^{-1}=A^{\\top}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 42**)\n",
    "+ 역행렬은 많은 계산량이 필요하나, 전치행렬은 계산량이 적게 소모되어, 이를 응용 가능\n",
    "+ There is no special term for a matrix whose rows or columns are orthogonal but not orthonormal. (무슨의미?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 많은 수학적 object는 그것들을 부분으로 쪼개거나 일부 속성들을 표현함으로써 더 잘 이해할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 예를 들어 정수는 소인수 분해 하여 소수들의 곱으로 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬도 마찬가지로 행렬의 열벡터들이 표현 가능한 공간의 기저가 되는 부분들을 뽑아낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 이를 위한 분해 기법 중 하나를 고유값 분해(Eigen decomposition)라고 부르며, 여기에서 고유값 분해를 통해 **고유 벡터**과 **고유값**을 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ 정사각행렬 $A$의 고유 벡터를 $v$라 한다면, $A$와 $v$를 곱하면 단지 $v$의 스케일만 변경되게 된다.\n",
    "+ 서로 다른 행렬 $A$와 $B$를 곱하면 전혀 관계 없는 $C$가 나오지만, $A$의 고유 벡터를 곱하게 되면 관계 없는 $C$가 나오는게 아니라 단지 $\\lambda$만큼의 스케일 변화가 된 $v$가 나오게 된다.\n",
    "$$Av = \\lambda v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 여기서 $\\lambda$는 고유값이라 한다.\n",
    "+ $v^{\\top}A = \\lambda v^{\\top}$ 과 같은 왼쪽 고유벡터를 구할수도 있다. (계속 열벡터를 중심으로 설명 했지만, 행벡터를 계산한다는 뜻)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p8:\n",
    "+ $s \\in \\mathbb{R}, s \\neq 0$를 만족하는 어떠한 상수 $s$를 곱해도 고유벡터는 $v$로, 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p9:\n",
    "+ 행렬 $A$에 대해 $n$개의 선형 독립적인 고유벡터들이 존재 할 때, 모든 고유벡터들을 연결하여 행렬 $V$를 만들 수 있다.\n",
    "+ 같은 방법으로 각각의 고유벡터들에 상응하는 고유값 $\\lambda$들을 연결하여 벡터를 만들 수 있고, 이를 $\\lambda$라고 하면, 대각행렬 $diag(\\lambda)$를 만들 수 있다.\n",
    "+ 이들을 이용한 고유값 분해는 다음과 같이 주어진다.\n",
    "\n",
    "$$A = V diag(\\lambda)V^{-1}$$\n",
    "\n",
    "+ 위와 같은 식을 유도 해 내기 위해서는 행렬식 개념을 알아야 하며, 꽤나 복잡한 과정(단순 산술 연산이 매우 많음)의 수식을 거쳐야한다. 하지만 이 책에서 고유값 분해의 과정 자체는 중요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p10:\n",
    "+ 고유값과 구유벡터로 행렬을 분해 하면, 원하는 방향으로 공간을 늘리는 것과 같은 응용 동작을 쉽게 설명할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p11:\n",
    "+ 모든 행렬들이 고유값 분해가 되지는 않는다.\n",
    "+ 만약 $A$가 대칭 행렬일 경우 다음과 같이 정리될 수 있다.\n",
    "\n",
    "$$ A = Q \\Lambda Q^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 44**)\n",
    "+ 여기서 $Q$는 $A$의 고유벡터의 직교행렬이고, $\\Lambda$ 는 $diag(\\lambda)$이다.\n",
    "+ $Q$는 직교행렬 이므로 각 $i$번째 열벡터 성분마다 $\\lambda_i$만큼 늘린 것으로 볼 수 있다.\n",
    "![add_2_5](https://user-images.githubusercontent.com/52661707/75309832-7f758100-5895-11ea-8eff-b6031c9c4937.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ $A$의 고유 벡터들은 중복이 될 수 있다. \n",
    "+ 이 경우는 2차 방정식의 끝 값이 $y=0$ 를 지날 때 해가 1개 인 것 처럼, 고유값 분해에서도 중복되는 고유벡터들이 나올 가능성이 존재 한다는 뜻 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ 고유값중 하나가 0인 경우 영공간이 존재하며, **'특이'**한 경우로 판단된다.\n",
    "+ 예를 들어, 대칭행렬 $A$에 대해 고유값이 3개 나오며, 이 중 1개의 값이 0인 경우에는 행렬 $A$는 3개의 3차원 벡터로 이루어져있지만, 하나의 차원은 영공간이 되므로 2차원 만 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 고유값이 모두 양수인 경우 **positive definite**라 한다.\n",
    "+ 고유값이 모두 양수이거나 모두 0인 경우 **positive semidefinite**라 한다.\n",
    "+ 고유값이 모두 음수인 경우 **negative definite**라 한다.\n",
    "+ 고유값이 모두 음수이거나 모두 0인 경우 **negative semidefinite**라 한다.\n",
    "+ [대칭행렬의 고유값 부호](https://datascienceschool.net/view-notebook/7fd58178d9e64be682058db7e024d8b5/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singlar Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ SVD (Singular Value Decomposition) 또한 행렬을 분해하는 방법 중 하나 이다.\n",
    "+ SVD가 더 많은 경우에도 동작한다. 특히 정사각행렬이 아니어도 분해 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 (**page 45**)\n",
    "+ 고유값 분해는 다음의 2개의 행렬로 표현될 수 있다.\n",
    "\n",
    "$$A = V diag(\\lambda)V^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2:\n",
    "+ 특이값 분해는 3개의 행렬로 표현된다.\n",
    "\n",
    "$$A = UDV^{\\top}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3:\n",
    "+ $A$는 m x n 행렬이라고 가정 하면, $U$는 m x m, $D$는 m x n, $V$는 n x n으로 정의된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4:\n",
    "+ 행렬 $U$와 $V$는 직교 유니터리 행렬로 정의된다.\n",
    "+ 유니터리 행렬이란 켤레 전치가 역행렬과 같은 복소수 행렬이다.\n",
    "+ 행렬 $D$는 대각행렬 이다. 또한 반드시 정사각행렬 일 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5:\n",
    "+ 행렬 $D$의 대각 성분은 $A$의 특이값으로 구성된다.\n",
    "+ 행렬 $U$의 열은 좌측 특이벡터로 이루어져있다.\n",
    "+ 행렬 $V$의 열은 우측 특이벡터로 이루어져있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6:\n",
    "+ $A$의 좌측 특이벡터는 $AA^{\\top}$의 고유벡터와 같다.\n",
    "+ $A$의 우측 특이벡터는 $A^{\\top}A$의 고유벡터와 같다.\n",
    "+ $A$의 0이 아닌 특이값은 $AA^{\\top}$의 고유값의 제곱근과 같다. $A^{\\top}A$와도 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p7:\n",
    "+ 특이값 분해에 대한 유용한 기능들은 다음 세션에 소개된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 The Moore-Penrose Pseudoinverse \n",
    "p1.(**page. 45**)\n",
    "- 행렬의 역행렬은 정방행렬이 아니면 정의될 수 없음\n",
    "- 행렬 A의 left-inverse B를 구한다고 할 때 다음의 선형 방정식을$${Ax=y}$$ \n",
    "- 양 변을 left-multiplying을 통해 풀 수 있음$${x=By}$$\n",
    "\n",
    "p2.\n",
    "- 이 문제의 구조 상 행렬 A를 B로 mapping 시키는 디자인은 불가능 (왜?)\n",
    "\n",
    "p3.\n",
    "- 행렬 A의 행의 크기가 열의 크기보다 클 경우, 방정식의 해가 존재하지 않음\n",
    "    - 조건이 너무 많은 경우\n",
    "- 열의 크기가 행의 크기보다 클 경우, 다수의 해가 존재\n",
    "    - 조건이 너무 적은 경우\n",
    "\n",
    "<img width=\"672\" alt=\"add_2_6\" src=\"https://user-images.githubusercontent.com/52661707/75309834-800e1780-5895-11ea-9cbf-b6a92e0e1d78.PNG\">\n",
    "\n",
    "https://twlab.tistory.com/31\n",
    "\n",
    "p3.\n",
    "- Moore-Penrose 의사역행렬은 위의 상황을 해결할 수 있음\n",
    "- 즉, 유일한 해가 존재하지 않는 선형연립방정식에서 최적해를 구하기 위해 사용\n",
    "\n",
    "- 행렬 A의 의사 역행렬은 다음과 같이 정의\n",
    "\n",
    "$${A^+=\\lim_{a \\to 0}(\\mathbf{A}^\\intercal A+\\alpha I)^{-1}\\mathbf{A}^\\intercal}$$\n",
    "\n",
    "- 실용적인 알고리즘은 오히려 다음의 수식을 따름\n",
    "\n",
    "$${A^+=VD^+\\mathbf{U}^\\intercal=Vdiag_{n,m}(\\lambda_1^+,...,\\lambda_{min{(m,n)}}^+)U^*}$$\n",
    "\n",
    "- U, D, V는 행렬 A의 SVD, D의 의사역행렬${D^+}$는 대각 행렬 D의 non-zero 요소들을 역수를 취한 행렬을 transpose하여 구할 수 있음\n",
    "\n",
    "p4.\n",
    "- 행렬 A에 대해 underdetermined일 경우, 의사 역행렬을 이용하여 선형 방정식 해를 많이 구할 수 있음\n",
    "- 해${x=A^+y}$를 모든 가능한 해 중에 최소의 유클리디안 norm ${||{x}||_{2}}$으로 구할 수 있음\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/9982B7505BD9562015\" style=\"width: 400px;\">\n",
    "https://freshrimpsushi.tistory.com/257\n",
    "\n",
    "p5.\n",
    "- 행렬 A에 대해 overdetermined일 경우 해를 구할 수 없어 의사역행렬을 이용\n",
    "- 유클리드 norm ${|\\left\\lvert{Ax-y}\\right\\rvert|_{2}}$에 대해 y값에 최대한 가까운 해 x를 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 The Trace Operator \n",
    "p1.(**page. 46**)\n",
    "- 행렬의 대각선 요소들의 합을 구하는 대각합 연산자$${Tr(A)=\\sum_{i}A_{i,i}}$$\n",
    "\n",
    "p2.\n",
    "- 합 계산 없이는 특정할 수 없는 연산들은 행렬의 곱과 대각합 연산을 이용하여 표현할 수 있음\n",
    "- 예) 행렬의 Frobenius norm을 작성하는 대안을 제공함\n",
    "\n",
    "$${||{Ax-y}||_{F}=\\sqrt{Tr(A\\mathbf{A}^\\intercal)}}$$\n",
    "\n",
    "(Frobenius norm: 유클리디안 놈에 종속된 consistent matrix, ${p=2}$)\n",
    "\n",
    "p3.\n",
    "- 대각합은 대각합의 transpose와 동일$${Tr(A)=Tr(\\mathbf{A}^\\intercal)}$$\n",
    "\n",
    "p4.\n",
    "- 대각합은 행렬곱의 조건을 만족하는 factor의 순서를 바꾸어도 결과는 변하지 않음\n",
    "\n",
    "$${Tr(ABC)=Tr(CAB)=Tr(BCA)}$$\n",
    "\n",
    "- 일반적으로\n",
    "\n",
    "$${Tr(\\prod_{i=0}^{n}F^{(i)})=Tr(F^{(n)}\\prod_{i=0}^{n}F^{(i-1)})}$$\n",
    "\n",
    "- 결과 곱의 형태가 다른 경우에도 원순열에 대한 불변성은 유지됨\n",
    "- 예)${A \\in R^{m*n}}$, ${B \\in R^{m*n}}$에 대하여 ${AB \\in R^{m*m}}$, ${BA \\in R^{n*n}}$의 조건이라 할지라도 ${Tr(AB)=Tr(BA)}$가 성립\n",
    "\n",
    "p5.\n",
    "- 스칼라 값 자체의 대각합: ${a=Tr(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 The Determinant \n",
    "p1.(**page. 47**)\n",
    "- 정방행렬의 determinant란 실제 스칼라값에 function을 mapping하는 행렬로${det(A)}$로 정의됨\n",
    "- 행렬의 모든 eigenvalue의 곱과동일\n",
    "- determinant의 절대값은 행렬의 곱셈이 공간을 얼마나 확장하고 축소하는지를 나타냄\n",
    "\n",
    "    1. determinant = 0, 역행렬 X\n",
    "    2. determinant != 0, 역행렬 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 Example: Principal Components Analysis \n",
    "p1. (**page. 48**)\n",
    "- 간단한 머신 러닝 알고리즘인 주성분 분석 (principal components analysis)은 기본 선형대수학 지식만으로도 유도할 수 있음\n",
    "\n",
    "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F99FF9F335B8A484A31820B)\n",
    "\n",
    "https://excelsior-cjh.tistory.com/167\n",
    "\n",
    "p2.\n",
    "- 집합 ${R^n}$에 ${m}$개의 점 ${\\{x^{(1)},...,x^{(m)}}\\}$이 있을 때 손상 압축을 적용\n",
    "- 손상 압축(lossy compression)이란 더 적은 메모리를 요구하지만 정확도가 저하될 수 있는 형식의 points 저장 방식\n",
    "\n",
    "p3.\n",
    "- 이 점들을 encode하기 위해서 더 낮은 차원을 이용\n",
    "- ${x^{(i)} \\in R^n}$의 각 점에 대응되는 code vector ${c^{(i)} \\in R^l}$에 대해서\n",
    "- l < n, 초기 데이터보다 더 적은 저장공간을 필요로 하는 ${x -> code}$ encoding function ${f(x)=c}$,\n",
    "- ${code -> x}$로 재구성하는 function ${x \\approx g(f(x))}$이 필요\n",
    "\n",
    "p4.\n",
    "- PCA는 decoding function에 의해 정의됨\n",
    "- decode가 단순하기 위해서 행렬 곱셈을 사용하여 code를 ${R^n}$으로 mapping\n",
    "- decoding function ${g(c)=Dc}$라고 할 때 decoding 행렬 D는 ${D \\in R^{n*l}}$\n",
    "\n",
    "p5.\n",
    "- decoding 문제를 쉽게하기 위해 PCA는 행렬 D의 열이 서로 orthogonal하게 제약\n",
    "- 이 때 행렬 D는 ${l=n}$이 아닌 이상 \"수직 행렬\"이 아님\n",
    "\n",
    "p6.\n",
    "- 모든 점에 대해 code ${c_{i}}$를 비례하게 감소시킨다면 ${D_{:,i}}$의 sclae을 증가시킬 수 있음\n",
    "- unique한 해결을 위해 행렬 D의 모든 열은 단위 norm으로 제약\n",
    "\n",
    "p7.\n",
    "- 알고리즘으로 나타내기 위해 먼저 각 input point ${x}$에 대해 optimal code point ${c^*}$을 어떻게 생성하는지 알아야함\n",
    "- norm을 이용하여 input ${x}$와 재구성 ${g(c^*)}$의 거리를 측정하고 최소화 하는 것\n",
    "- 주성분 알고리즘에 ${L^2}$ norm을 이용$${c^*=\\underset{c}{\\operatorname{argmin}}||{x-g(c)}||_2}$$\n",
    "\n",
    "p8.\n",
    "- ${L^2}$ norm을 제곱 ${L^2}$ norm으로 교체가능 (두 norm 모두 같은 c값으로 최소화되기 때문)\n",
    "    - ${L^2}$은 음수가 아니고 제곱 연산은 음이 아닌 인수에 대한 감소가 아닌 증가\n",
    "    \n",
    "$${c^*=\\underset{c}{\\operatorname{argmin}}||{x-g(c)}||_2^2}$$\n",
    "\n",
    "- 최소화 되는 함수는 다음과 같이 단순화할 수 있고\n",
    "\n",
    "$${\\mathbf(x-g(c))^\\intercal(x-g(c))}$$\n",
    "\n",
    "- ${l^2}$의 정의와 분배 법칙에 의해 다음과 같이 정리\n",
    "\n",
    "$${\\mathbf{x}^\\intercal x-2\\mathbf{x}^\\intercal g(c)+\\mathbf{g(c)}^\\intercal g(c)}$$\n",
    "\n",
    "- ${\\mathbf{x}^\\intercal x}$를 생략하기 위해 다시 한번 최소화\n",
    "\n",
    "$${c^*=\\underset{c}{\\operatorname{argmin}}-2\\mathbf{x}^\\intercal g(c)+\\mathbf{g(c)}^\\intercal g(c)}$$\n",
    "\n",
    "- ${g(c)}$의 정의를 ${D}$로 대체하고 직교성과 행렬 ${D}$의 단위 norm 제약사항에 의하여 정리\n",
    "\n",
    "$${c^*=\\underset{c}{\\operatorname{argmin}}-2\\mathbf{x}^\\intercal Dc+\\mathbf{c}^\\intercal c}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p9.\n",
    "- section 4.3의 'vector calculus'를 사용하여 이 최적화 문제를 해결\n",
    "\n",
    "$${\\nabla_{c}(-2\\mathbf{x}^\\intercal Dc+\\mathbf{c}^\\intercal c)=0}$$\n",
    "$${c=\\mathbf{D}^\\intercal x}$$\n",
    "\n",
    "p10.\n",
    "- matrix-vector 연산으로 ${x}$를 optimal하게 encode 가능\n",
    "\n",
    "- vector를 encode하기 위해 encoder function을 적용$${f(x)=\\mathbf{D}^\\intercal x}$$\n",
    "- 추가적인 행렬 곱셈을 하기 위해 PCA reconstruction을 정의\n",
    "\n",
    "$${r(x)=g(f(x))=D\\mathbf{D}^\\intercal x}$$\n",
    "\n",
    "p11.\n",
    "- encoding matrix ${D}$ 설정\n",
    "- input과 reconstruction 사이의 ${L^2}$ 거리를 촤소화\n",
    "- 모든 점을 ${D}$로 decode할 것이기 때문에 고립점을 고려하는 대신 모든 차원과 모든 점들에 걸쳐 계산된 오차 행렬의 Frobenius norm을 최소화해야함\n",
    "\n",
    "$${D^*={\\underset{D}{\\operatorname{argmin}}\\sqrt{\\sum_{i,j}(x_j^{(i)}-r({x^{(i)})_j}^2)^2}}}$$\n",
    "\n",
    "p12.\n",
    "- ${D^*}$를 유도하기 위해 ${l=1}$인 경우를 고려\n",
    "- ${l=1}$일 때 encoding 행렬 ${D}$는 single vector ${d}$이고 p10의 ${r(x)}$를 p11의 식에 대입하여 ${D}$를 ${d}$로 단순화하여 문제 감소\n",
    "\n",
    "$${d*=\\underset{d}{\\operatorname{argmin}}\\sum_{i}||{x^{(i)}-d\\mathbf{d}^\\intercal x^{(i)}}||_2^2}$$\n",
    "\n",
    "p13.\n",
    "- 기존 스칼라 계수를 쓸 때 transpose 행렬 ${\\mathbf{d}^\\intercal x^{(i)}}$를 벡터 왼쪽에 표현\n",
    "\n",
    "$${d*=\\underset{d}{\\operatorname{argmin}}\\sum_{i}||{x^{(i)}-\\mathbf{d}^\\intercal x^{(i)}d}||_2^2}$$\n",
    "\n",
    "p14.\n",
    "- 또는 스칼라의 transpose는 기존 스칼라 값과 같은 성질을 이용하여 정리\n",
    "\n",
    "$${d*=\\underset{d}{\\operatorname{argmin}}\\sum_{i}||{x^{(i)}-\\mathbf{x^{(i)}}^\\intercal dd}||_2^2}$$\n",
    "- 이런 변형에 익숙해져야함\n",
    "\n",
    "p15.\n",
    "- 각 개별 벡터의 합보다 하나의 행렬 디자인으로 표현하는 것이 더 좋을 것이고 더 compact한 표현이 가능\n",
    "- ${X \\in R^{(m*n)}}$ 일 때, 행렬은 points를 나타내는 모든 벡터의 stack으로 표현 ${X_{i,:}=\\mathbf{x^{(i)}}^\\intercal}$\n",
    "- 다시 ${d}$에 대해 정리\n",
    "\n",
    "$${d^*=\\underset{d}{\\operatorname{argmin}}||{X-Xd\\mathbf{d}^\\intercal}||_F^2}$$\n",
    "- 제약사항 ${\\mathbf{d}^\\intercal d=1}$\n",
    "\n",
    "\n",
    "- 이 시점에서의 제약사항을 고려하지 않고 Frobenius norm portion을 다음과 같이 정리\n",
    "\n",
    "$${\\underset{d}{\\operatorname{argmin}}||{X-Xd\\mathbf{d}^\\intercal}||_F^2=\\underset{d}{\\operatorname{argmin}}Tr(\\mathbf(X-Xd\\mathbf{d}^\\intercal)^\\intercal (X-Xd\\mathbf{d}^\\intercal))}$$\n",
    "\n",
    "- page. 47 행렬의 Frobenius norm 식 (2.49), ${d}$를 포함하지 않는 항 소거 및 page. 47 대각행렬 교환법칙 식 (2.52)f를 이용하여 정리\n",
    "\n",
    "$${=\\underset{d}{\\operatorname{argmin}}-2Tr(\\mathbf{X}^\\intercal Xd\\mathbf{d}^\\intercal)+Tr(\\mathbf{X}^\\intercal Xd\\mathbf{d}^\\intercal d\\mathbf{d}^\\intercal)}$$\n",
    "\n",
    "- 제약사항 ${\\mathbf{d}^\\intercal d=1}$를 적용\n",
    "\n",
    "$${=\\underset{d}{\\operatorname{argmin}}-Tr(\\mathbf{X}^\\intercal Xd\\mathbf{d}^\\intercal)}$$\n",
    "\n",
    "$${=\\underset{d}{\\operatorname{argmax}}Tr(\\mathbf{X}^\\intercal Xd\\mathbf{d}^\\intercal)}$$\n",
    "\n",
    "$${=\\underset{d}{\\operatorname{argmax}}Tr(\\mathbf{d}^\\intercal\\mathbf{X}^\\intercal Xd)}$$\n",
    "\n",
    "p16.\n",
    "- 이와 같은 최적화 문제는 eigendecomposition을 통해 해결 가능할 수 있음\n",
    "- optimal ${d}$는 ${\\mathbf{X}^\\intercal X}$의 가장 큰 eigenvalue에 대응되는 eigenvector에서 주어짐\n",
    "\n",
    "p17.\n",
    "- 이러한 유도는 ${l=1}$의 경우에 specific하고 첫 번째 주성분만 복구\n",
    "- 더 일반적으로, 주성분의 기저를 복구할 때 행렬 ${D}$는 가장 큰 eigenvalues에 대응되는 ${l}$ eigenvectors로부터 주어짐\n",
    "- 귀납법으로 증명가능 (연습 권장)\n",
    "\n",
    "p18.\n",
    "- 선형대수학은 deep learning을 이해하는데 기본적인 수학적 지식 중 하나\n",
    "- machine learning을 위한 핵심 수리 영역은 다음 장에 나올 확률 이론임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
